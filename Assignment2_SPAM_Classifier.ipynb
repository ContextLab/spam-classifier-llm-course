{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ContextLab/spam-classifier-llm-course/blob/main/Assignment2_SPAM_Classifier.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Advanced SPAM Classifier with Multi-Method Comparison\n",
    "\n",
    "**Course:** PSYC 51.17 - Models of Language and Communication  \n",
    "**Deadline:** January 26, 2026 at 11:59 PM EST\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this assignment, you will build a comprehensive spam classification system that implements multiple classification approaches, conducts rigorous comparative analysis, and performs extensive error analysis.\n",
    "\n",
    "Please refer to the [full assignment instructions](https://contextlab.github.io/llm-course/assignments/assignment-2/) for detailed requirements and grading rubric.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Data Loading](#setup)\n",
    "2. [Part 1: Classifier Implementations](#part1)\n",
    "3. [Part 2: Comprehensive Evaluation](#part2)\n",
    "4. [Part 3: Error Analysis](#part3)\n",
    "5. [Part 4: Adversarial Testing](#part4)\n",
    "6. [Part 5: Real-World Considerations](#part5)\n",
    "7. [Discussion and Reflection](#discussion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets scikit-learn pandas numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the training dataset\n",
    "dataset_url = 'https://raw.githubusercontent.com/ContextLab/spam-classifier-llm-course/main/training.zip'\n",
    "dataset_path = 'training.zip'\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    print(\"Downloading dataset...\")\n",
    "    urllib.request.urlretrieve(dataset_url, dataset_path)\n",
    "    print(\"Download complete.\")\n",
    "else:\n",
    "    print(\"Dataset already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(zip_path):\n",
    "    \"\"\"\n",
    "    Load emails from a zip archive containing spam/ and ham/ folders.\n",
    "    \n",
    "    Returns:\n",
    "        emails: List of email texts\n",
    "        labels: List of labels (1 for spam, 0 for ham)\n",
    "    \"\"\"\n",
    "    # Extract if needed\n",
    "    dataset_dir = Path(zip_path).with_suffix('')\n",
    "    if not dataset_dir.exists():\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(dataset_dir)\n",
    "    \n",
    "    emails = []\n",
    "    labels = []\n",
    "    \n",
    "    # Load spam\n",
    "    spam_folder = dataset_dir / \"spam\"\n",
    "    for file_path in spam_folder.iterdir():\n",
    "        if file_path.is_file():\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                emails.append(f.read())\n",
    "                labels.append(1)\n",
    "    \n",
    "    # Load ham\n",
    "    ham_folder = dataset_dir / \"ham\"\n",
    "    for file_path in ham_folder.iterdir():\n",
    "        if file_path.is_file():\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                emails.append(f.read())\n",
    "                labels.append(0)\n",
    "    \n",
    "    return emails, labels\n",
    "\n",
    "emails, labels = load_dataset('training.zip')\n",
    "print(f\"Loaded {len(emails)} emails\")\n",
    "print(f\"Spam: {sum(labels)}, Ham: {len(labels) - sum(labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split data into train/validation/test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part1'></a>\n",
    "## 2. Part 1: Multiple Classifier Implementations (40 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Traditional ML Baseline (15 points)\n",
    "\n",
    "Implement **two** traditional ML classifiers with TF-IDF features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# TODO: Implement TF-IDF vectorizer with appropriate parameters\n",
    "# Document your feature engineering choices\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    # Your parameters here\n",
    ")\n",
    "\n",
    "# TODO: Implement first traditional classifier\n",
    "# classifier1 = ...\n",
    "\n",
    "# TODO: Implement second traditional classifier\n",
    "# classifier2 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Neural/Transformer-Based Model (15 points)\n",
    "\n",
    "Implement a transformer-based classifier (BERT, DistilBERT, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# TODO: Load pre-trained model and tokenizer\n",
    "# model_name = 'distilbert-base-uncased'  # Recommended for speed\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# TODO: Implement fine-tuning on spam data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Ensemble Method (10 points)\n",
    "\n",
    "Create an ensemble that combines your best models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement ensemble method (voting, stacking, or boosting)\n",
    "# Document your ensemble strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part2'></a>\n",
    "## 3. Part 2: Comprehensive Evaluation (25 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
    "\n",
    "def evaluate_classifier(y_true, y_pred, y_prob=None, name=\"Classifier\"):\n",
    "    \"\"\"\n",
    "    Compute and display all required metrics for a classifier.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{name} Evaluation\")\n",
    "    print('='*50)\n",
    "    \n",
    "    print(f\"Accuracy:  {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"Recall:    {recall_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"F1 Score:  {f1_score(y_true, y_pred):.4f}\")\n",
    "    \n",
    "    if y_prob is not None:\n",
    "        print(f\"AUC-ROC:   {roc_auc_score(y_true, y_prob):.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['Ham', 'Spam']))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])\n",
    "    plt.title(f'{name} - Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create comparison table showing all metrics for all classifiers\n",
    "# Include: Accuracy, Precision, Recall, F1, AUC-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Measure computational efficiency\n",
    "# Training time, inference time, model size, throughput\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Cross-validation with statistical significance testing\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part3'></a>\n",
    "## 4. Part 3: Error Analysis (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Identify and analyze at least 20 misclassified emails\n",
    "# - 10 false positives (ham classified as spam)\n",
    "# - 10 false negatives (spam classified as ham)\n",
    "\n",
    "# Categorize errors and analyze patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Feature importance analysis\n",
    "# What words/patterns are most predictive?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part4'></a>\n",
    "## 5. Part 4: Adversarial Testing (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create at least 5 adversarial emails\n",
    "# Test on all classifiers and analyze which are most robust\n",
    "\n",
    "adversarial_emails = [\n",
    "    # Example: spam trying to evade detection\n",
    "    \"\"\"Fr33 V1agra! Click here for amazing deals!\n",
    "    \n",
    "    The weather today is quite nice. I hope you are doing well.\n",
    "    Best regards from a legitimate sender.\"\"\",\n",
    "    \n",
    "    # Add more adversarial examples\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test robustness against perturbations\n",
    "# - Typos and misspellings\n",
    "# - Case variations\n",
    "# - Synonym replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part5'></a>\n",
    "## 6. Part 5: Real-World Considerations (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance Discussion\n",
    "\n",
    "(How did you handle class imbalance? What happens if spam/ham ratio changes in production?)\n",
    "\n",
    "### Deployment Scenarios\n",
    "\n",
    "Which model would you choose for:\n",
    "- **Mobile email app** (fast inference, small size): \n",
    "- **Email server** (high throughput): \n",
    "- **Maximum accuracy** (no constraints): \n",
    "\n",
    "(Justify with evidence from your experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='discussion'></a>\n",
    "## 7. Discussion and Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings\n",
    "\n",
    "(What did you learn about spam classification? Which approach worked best and why?)\n",
    "\n",
    "### Limitations\n",
    "\n",
    "(What are the limitations of your approach? What would you improve with more time?)\n",
    "\n",
    "### Reflection\n",
    "\n",
    "(What was challenging? What surprised you about the results?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Submission Checklist\n",
    "\n",
    "- [ ] At least 3 classifiers implemented (2 traditional + 1 neural + ensemble)\n",
    "- [ ] All metrics computed and comparison table created\n",
    "- [ ] Cross-validation with statistical testing\n",
    "- [ ] At least 20 error cases analyzed\n",
    "- [ ] At least 5 adversarial examples created and tested\n",
    "- [ ] Deployment recommendations with justification\n",
    "- [ ] Discussion and reflection complete\n",
    "- [ ] Notebook runs without errors\n",
    "\n",
    "**To submit:** Commit and push this notebook to your GitHub repository before the deadline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}